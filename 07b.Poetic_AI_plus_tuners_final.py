# -*- coding: utf-8 -*-
"""NAI_poetic_AI_plus_tuners_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pIdfAalxl4kpKoolbcLaqQZ35ZU3iWfI

BartoÅ› Edyta, Marcin Grelewicz,
LSTM generating poems
As inspiration we used: https://medium.com/predict/creating-a-poem-writer-ai-using-keras-and-tensorflow-16eac157cba6

Import bibliotek
"""

import keras
import tensorflow as tf
from sklearn import model_selection
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers
import tensorflow.keras.utils
import numpy as np

from google.colab import drive
drive.mount('/content/drive/')

"""Tokenizing"""

# with keras preprocessing we vectorize our words
tokenizer = Tokenizer()

# load data
data = open('/content/drive/MyDrive/ai_poeta/poems.txt', encoding="utf8").read()

print(tokenizer)

#print(data)

#we lowercase and we call split method to treat sentence as a sequence
text = data.lower().split("\n")

print(text)

tokenizer.fit_on_texts(text)
'''The fit_on_texts method takes one sentence at a time, converts that sentence 
into the number sequence of each word from the dictionary and gives you back the
 final list of sequence.'''

total_words = len(tokenizer.word_index) + 1 # to avoid oov token
'''total number of words and add the length + 1 just to include a word tag which 
indicates if the new word coming in during testing is in the dictionary or not.'''

print(total_words)

tokenized_text = [] #input vector which we feed the network
'''The for loop goes sequence by sequence(line by line) and generates new n-gram
 sequences and appends them in our training data.'''
for line in text:
	token_list = tokenizer.texts_to_sequences([line])[0] #give text of the sequences for the current line
	for i in range(1, len(token_list)):
		n_gram_sequence = token_list[:i+1]
		tokenized_text.append(n_gram_sequence)

# pad sequences 
max_sequence_len = max([len(item) for item in tokenized_text])
tokenized_text = np.array(pad_sequences(tokenized_text, maxlen=max_sequence_len, padding='pre'))

# create X and y
X, y = tokenized_text[:,:-1],tokenized_text[:,-1]
print(X.shape, y.shape)
#X-> beginning of the list
#y-> end of the list

'''convert targets to categorical y's i.e one hot encode them and treat the
 total number of words in the dictionary as the total number of classes.'''
y = tensorflow.keras.utils.to_categorical(y, num_classes=total_words)

print(X.shape, y.shape)

from sklearn.model_selection import train_test_split
# split into train test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

"""Keras Tuner"""

!pip install -q -U keras-tuner

import kerastuner

hp = kerastuner.HyperParameters()

def build_model(hp):
  model = Sequential()
  model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))
  model.add(Bidirectional(LSTM(150, return_sequences = True)))
  model.add(Dropout(0.2))
  model.add(LSTM(100))
  #model.add(Bidirectional(LSTM(10, return_sequences = True)))
  #model.add(Dropout(0.1))
  #model.add(LSTM(10))
  '''model.add(Dense(
      hp.Int('hidden units',
            min_value=50,
            max_value = 200,
            step = 50),
      activation = 'relu'))'''
  model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
  model.add(Dense(total_words, activation='softmax'))

  model.compile(
    #optimizer=keras.optimizers.SGD(),
    optimizer='adam',
    #loss='sparse_categorical_crossentropy',
    loss='categorical_crossentropy',
    metrics=['accuracy'])
  return model

"""Random Search"""

from kerastuner.tuners import RandomSearch

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=4,
    executions_per_trial=1,
    directory='experiments',
    project_name='test')

tuner.search_space_summary()

history = tuner.search(x = X_train, y = y_train, validation_data=(X_test, y_test), epochs = 200)

tuner.get_best_models(num_models=2)

tuner.results_summary()

"""BayesianOptimization"""

def build_model(hp):
  model = Sequential()
  model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))
  '''the model will take as input an integer matrix of size (batch, input_length). 
  the largest integer (i.e. word index) in the input should be
  no larger than 999 (vocabulary size). 
  now model.output_shape == (None, 10, 64), where None is the batch dimension.'''
  model.add(Bidirectional(LSTM(150, return_sequences = True)))
  model.add(Dropout(0.2)) #to avoid overfitting
  model.add(LSTM(100))
  '''model.add(Dense(
      hp.Int('hidden units',
            min_value=50,
            max_value = 200,
            step = 50),
      activation = 'relu'))'''
  model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01))) #regulizer to avoid overfitting
  # final dense layer => the size of total words to predict
  model.add(Dense(total_words, activation='softmax'))

  model.compile(
    optimizer='adam', #with default learnig rate 
    loss='categorical_crossentropy', #our loss function
    metrics=['accuracy'])
  return model

from kerastuner.tuners import BayesianOptimization

tuner = BayesianOptimization(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=1,
    directory='experiments',
    project_name='test2')

tuner.search_space_summary()

history = tuner.search(x = X_train, y = y_train, validation_data=(X_test, y_test), epochs = 10)

tuner.get_best_models(num_models=2)

tuner.results_summary()

'''my_callbacks = [
    #tf.keras.callbacks.EarlyStopping(patience=2),
    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),
    tf.keras.callbacks.TensorBoard(log_dir='./logs'),
]'''

model=build_model(hp)

#history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 200, callbacks=my_callbacks)
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 300)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

#save model
model.save('/content/drive/MyDrive/ai_poeta/', overwrite=True, include_optimizer=True)

#load our saved model
import keras # we repeat only for loading our saved model case
model = keras.models.load_model('/content/drive/MyDrive/ai_poeta/')

"""Generate"""

starting_word = "Heart pain" #Put here some  sequence you want our AI to start from...
words_number = 30 #you can change this number, if you prefer longer or shorter 
#text to be generated
  
for _ in range(words_number):
	token_list = tokenizer.texts_to_sequences([starting_word])[0]
	token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
	predicted = model.predict_classes(token_list, verbose=0)
	output_word = ""
	for word, index in tokenizer.word_index.items():
		if index == predicted:
			output_word = word
			break
	starting_word += " " + output_word

from collections import Counter

#y = starting_word.split(" ")
    
#print(starting_word.replace(' ', '\n'))

for i in range(0,30,5):
    print(starting_word.split(' ')[i], starting_word.split(' ')[i+1],
          starting_word.split(' ')[i+2], starting_word.split(' ')[i+3],
          starting_word.split(' ')[i+4])